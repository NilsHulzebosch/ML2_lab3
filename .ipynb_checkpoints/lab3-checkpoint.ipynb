{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a10081f57b90a368eb8daf62e3ba00e",
     "grade": false,
     "grade_id": "cell-02487845739eb4fd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Lab 3: Expectation Maximization and Variational Autoencoder\n",
    "\n",
    "### Machine Learning 2 (2017/2018)\n",
    "\n",
    "* The lab exercises should be made in groups of two or three people.\n",
    "* The deadline is Friday, 01.06.\n",
    "* Assignment should be submitted through BlackBoard! Make sure to include your and your teammates' names with the submission.\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file should be \"studentid1\\_studentid2\\_lab#\", for example, the attached file should be \"12345\\_12346\\_lab1.ipynb\". Only use underscores (\"\\_\") to connect ids, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask.\n",
    "* Use __one cell__ for code and markdown answers only!\n",
    "    * Put all code in the cell with the ```# YOUR CODE HERE``` comment and overwrite the ```raise NotImplementedError()``` line.\n",
    "    * For theoretical questions, put your solution using LaTeX style formatting in the YOUR ANSWER HERE cell.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Large parts of you notebook will be graded automatically. Therefore it is important that your notebook can be run completely without errors and within a reasonable time limit. To test your notebook before submission, select Kernel -> Restart \\& Run All.\n",
    "$\\newcommand{\\bx}{\\mathbf{x}} \\newcommand{\\bpi}{\\mathbf{\\pi}} \\newcommand{\\bmu}{\\mathbf{\\mu}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\bZ}{\\mathbf{Z}} \\newcommand{\\bz}{\\mathbf{z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e4e05229ee79b55d6589e1ea8de68f32",
     "grade": false,
     "grade_id": "cell-a0a6fdb7ca694bee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Installing PyTorch\n",
    "\n",
    "In this lab we will use PyTorch. PyTorch is an open source deep learning framework primarily developed by Facebook's artificial-intelligence research group. In order to install PyTorch in your conda environment go to https://pytorch.org and select your operating system, conda, Python 3.6, no cuda. Copy the text from the \"Run this command:\" box. Now open a terminal and activate your 'ml2labs' conda environment. Paste the text and run. After the installation is done you should restart Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d9c3d77f550b5fd93b34fd18825c47f0",
     "grade": false,
     "grade_id": "cell-746cac8d9a21943b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### MNIST data\n",
    "\n",
    "In this Lab we will use several methods for unsupervised learning on the MNIST dataset of written digits. The dataset contains digital images of handwritten numbers $0$ through $9$. Each image has 28x28 pixels that each take 256 values in a range from white ($= 0$) to  black ($=1$). The labels belonging to the images are also included. \n",
    "Fortunately, PyTorch comes with a MNIST data loader. The first time you run the box below it will download the MNIST data set. That can take a couple of minutes.\n",
    "The main data types in PyTorch are tensors. For Part 1, we will convert those tensors to numpy arrays. In Part 2, we will use the torch module to directly work with PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fbc152afa1255331d7b88bf00b7156c",
     "grade": false,
     "grade_id": "cell-7c995be0fda080c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "train_labels = train_dataset.train_labels.numpy()\n",
    "train_data = train_dataset.train_data.numpy()\n",
    "# For EM we will use flattened data\n",
    "train_data = train_data.reshape(train_data.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fc852f9bfb0bab10d4c23eada309e89",
     "grade": false,
     "grade_id": "cell-8b4a44df532b1867",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Expectation Maximization\n",
    "We will use the Expectation Maximization (EM) algorithm for the recognition of handwritten digits in the MNIST dataset. The images are modelled as a Bernoulli mixture model (see Bishop $\\S9.3.3$):\n",
    "$$\n",
    "p(\\bx|\\bmu, \\bpi) = \\sum_{k=1}^K  \\pi_k \\prod_{i=1}^D \\mu_{ki}^{x_i}(1-\\mu_{ki})^{(1-x_i)}\n",
    "$$\n",
    "where $x_i$ is the value of pixel $i$ in an image, $\\mu_{ki}$ represents the probability that pixel $i$ in class $k$ is black, and $\\{\\pi_1, \\ldots, \\pi_K\\}$ are the mixing coefficients of classes in the data. We want to use this data set to classify new images of handwritten numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54064637b7e7cf938c0f778d748a226a",
     "grade": false,
     "grade_id": "cell-af03fef663aa85b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Binary data (5 points)\n",
    "As we like to apply our Bernoulli mixture model, write a function `binarize` to convert the (flattened) MNIST data to binary images, where each pixel $x_i \\in \\{0,1\\}$, by thresholding at an appropriate level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fe8607a4d734f7f26ef1ee1e54b33471",
     "grade": false,
     "grade_id": "cell-ec4365531ca57ef3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def binarize(X):\n",
    "    #print(np.mean(X, 1))\n",
    "    bin_X = copy.deepcopy(X)\n",
    "    bin_X[bin_X < np.mean(np.mean(X, 1))] = 0\n",
    "    bin_X[bin_X > np.mean(np.mean(X, 1))] = 1\n",
    "    return bin_X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "231b2c9f29bc5c536c60cef4d74793a1",
     "grade": true,
     "grade_id": "cell-2f16f57cb68a83b3",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test test test!\n",
    "bin_train_data = binarize(train_data)\n",
    "assert bin_train_data.dtype == np.float\n",
    "assert bin_train_data.shape == train_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a0a39404cc2f67078b399ee34653a3ac",
     "grade": false,
     "grade_id": "cell-462e747685e8670f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Sample a few images of digits $2$, $3$ and $4$; and show both the original and the binarized image together with their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3f3c981f0fda5ba3bdfcefb9144305c7",
     "grade": true,
     "grade_id": "cell-784c6bd177a9aa42",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAACSCAYAAACe94KvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABg9JREFUeJzt3U2oVHUYx/HvL8tFiJC9IWrpQiJXGRIFLlokmBvbBLpyIQhRUGALrV1tXLVzc0FRKIygQHcSEtQiwoooTXwpUC9KJhEVIiQ9LeYox/He5565M3Ne5v4+MMzMcZjzIL/7fzlz57mKCMxmc1/TBVi7OSCWckAs5YBYygGxlANiKQfEUg7IgCStlXRT0odN11IHB2Rw+4GTTRdRFwdkAJK2AX8CJ5qupS4OSEWSlgLvAbubrqVODkh17wMHIuJy04XU6f6mC+gCSc8ALwHrm66lbg5INS8Cq4FLkgCWAIskrYuIZxusa+zkj/vnJulBYGnp0Nv0AvNaRPzeSFE18QhSQUTcAG7cfi7pH+DmpIcDPILYHLyLsZQDYikHxFJDBUTSZklnJV2QtGdURVl7zHuRKmkRcA7YBEzT+wBre0T8PLryrGnDbHOfAy5ExK8Akj4GtgKzBkSSt0ztcT0iHp3rRcNMMSuA8ucS08Ux64aLVV40zAiiGY7dM0JI2gXsGuI81qBhAjINrCo9Xwlc6X9RREwBU+AppouGmWJOAmslrZG0GNgGHBtNWdYW8x5BIuKWpDeA48Ai4GBEnB5ZZdYKtX4W4ymmVb6LiA1zvchXUi3lgFjKAbGUA2IpB8RSDoilHBBL+ZeWOyC7VlV8DWNsPIJYygGxlANiKa9BWqhN31XyCGIpB8RSnmJaok3TSplHEEs5IJZyQCzlNUjHjPvSej+PIJZyQCzlKWYGdXx62tZtbT+PIJZyQCzlgFjKa5AB9a8d6th21r21LZtzBJF0UNI1SadKx5ZJ+lzS+eL+ofGWaU2pMsUcAjb3HdsDnIiItfT+NIb7k02oOQMSEV8Cf/Qd3gocLh4fBl4ZcV0TKSLu3LpivovUxyPiKkBx/9joSrI2Gfsi1S2oum2+I8hvkpYDFPfXZnthRExFxIYqvSisfeYbkGPAjuLxDuDoaMppTtX1gaS7buNQxzmqqrLNPQJ8DTwlaVrSTmAfsEnSeXqNdPeNt0xriltQFar+PwzzE13HOQZQqQWVr6SO0SA/fE1PJbPxZzGWckAs5YBYakGtQYb40ydjP0dbeQSxlANiqYmeYkY13JffZ1Tb0bZua/t5BLGUA2IpB8RSE7cGGfc2c5j378q6o8wjiKUcEEs5IJbq/BqkS5e2m2ypPV8eQSzlgFiq81PMIKoO43VMW22dUvp5BLGUA2IpB8RSE7cGafPc3ubaZuMRxFIOiKU6P8U0PWw3ff5x8whiqSpf3l4l6QtJZySdlvRmcdx9yhaAKiPILWB3RDwNPA+8Lmkd7lO2IFTpUXY1Ir4vHv8NnAFWMGF9ytrUH6RNBlqkSloNrAe+oa9PmaQZ+5S5BVW3VQ6IpCXAp8BbEfHXAB98TQFTxXt055c3DKi4i5H0AL1wfBQRnxWHK/cps+6qsosRcAA4ExEflP5p4vqU2b3mbEElaSPwFfAT8F9x+B1665BPgCeAS8CrEdHfcLf/vVo7xbSsPVQdKrWgco+yggMys85fap+vSegfVgdfareUA2IpB8RSDoilHBBLOSCWWrDb3IW8dR2ERxBLOSCWckAs5YBYygGxlANiKQfEUg6IpRwQSzkglnJALOWAWMoBsVTdn+ZeBy4CjxSP22Ch1vJklRfV+rWHOyeVvq3yK/d1cC05TzGWckAs1VRApho670xcS6KRNYh1h6cYS9UaEEmbJZ2VdEFS7T3NJB2UdE3SqdKxRprxdaU5YG0BkbQI2A+8DKwDthfN8Op0CNjcd6ypZnzdaA5Ybt42zhvwAnC89HwvsLeu85fOuxo4VXp+FlhePF4OnK27puLcR4FNbann9q3OKWYFcLn0fLo41rS7mvEBMzbjG6esOWAT9ZTVGZCZvqm04LdQ/c0Bm66nX50BmQZWlZ6vBK7UeP7ZNNaMrwvNAesMyElgraQ1khYD2+g1wmtaI834OtMcsOaF2BbgHPAL8G4DC8EjwFXgX3oj2k7gYXq7hfPF/bKaatlIb4r9EfihuG1pqp7Zbr6SailfSbWUA2IpB8RSDoilHBBLOSCWckAs5YBY6n8E0oKNPjl4XgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAACSCAYAAACe94KvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABk5JREFUeJzt3curVXUYxvHvk9bISXdE7UJIZYMIJAocNJGsBjYJdOTACKKgoInWP2CTBkENTiQKhREV1CCI6EIFEVZEaealwDp4UKSBQRBab4O9ktX2nPesfVuXvZ8PLPZey+1ZP+Q5v9+71t77VRGB2VIua3oA1m4OiKUcEEs5IJZyQCzlgFjKAbGUA1KRpNckLUg6J+mYpEebHlMd5Btl1Ui6AzgREX9Jug34FHgoIr5pdmST5Rmkoog4HBF//bdbbLc0OKRaOCADkPSypD+Bn4AF4P2GhzRxXmIGJGkFcC9wH/B8RJxvdkST5RlkQBHxd0R8AawFHm96PJPmgAxvJa5BDEDSdZK2SVolaYWk+4HtwMdNj23SXINUIOla4C3gTnq/VCeBFyPilUYHVgMHxFJeYizlgFjKAbHUSAGRtEXSUUknJO0a16CsPYYuUos7iseAzcA8cBDYHhE/jm941rSVI/zdu+m9u/kLgKQ3gK3AkgGR5Eum9jgbEdcu96JRlpg1wG+l/fnimHXDySovGmUG0SLHLpkhJD0GPDbCeaxBowRkHlhX2l8LnOp/UUTMAXPgJaaLRlliDgLrJd0s6QpgG/DeeIZlbTH0DBIRFyQ9CXwArAD2RsThsY3MWqHW92K8xLTKNxGxcbkX+U6qpRwQSzkglnJALOWAWMoBsZQDYikHxFIOiKUcEEs5IJYa5e3+2nTpuzvSYh+T6S7PIJZyQCzV2iWmS8tKWTbuLi4/nkEs5YBYygGxVGtrkGlUrk+6Uo94BrGUA2KpqVti6p66u3o5XpVnEEs5IJZyQCzV2hqkrZeB015z9Ft2BpG0V9IZSYdKx66S9KGk48XjlZMdpjWlyhKzD9jSd2wX8FFErAc+KvZtCi0bkIj4DPi97/BWYH/xfD/w8JjH1SoRcXEbhaSLW1cMW6ReHxELAMXjdeMbkrXJxItUt6DqtmFnkNOSVgMUj2eWemFEzEXExiq9KKx9hg3Ie8CO4vkO4N3xDKc55TqjfxtWueboUt1RVuUy9wDwJXCrpHlJO4E9wGZJx+k10t0z2WFaU9yCqjCJf4eWzxqVWlC19k7qpM3aHdFh+b0YSzkglnJALDWzNUgdBqlz2lrQegaxlANiKS8xLdHW78x4BrGUA2IpB8RSM1uDDLLO131bvv98TdYknkEs5YBYygGx1MzWIINoul5p8h6JZxBLOSCW8hIzZtkS0MVPsXkGsZQDYikHxFIOiKUcEEs5IJaaqcvcafufGOrgGcRSVb68vU7SJ5KOSDos6aniuPuUzYAqM8gF4JmIuB24B3hC0gbcp2wmVOlRthAR3xbP/wCOAGvoQJ+yrM9HHb07JtFnpG4DFamSbgLuAr6ir0+ZpEX7lLkFVbdVDoikVcDbwNMRca5qmiNiDpgrfkb33q2acZUCIulyeuF4PSLeKQ6flrS6mD3SPmV1qjqVj+uDwV18h3YQVa5iBLwKHImIF0p/NHV9yuxSy7agkrQJ+Bz4AfinOPwsvTrkTeAG4FfgkYjob7jb/7Mm/us27G90m2eQCRWnlVpQTV2PMgekMvcoG0Sbaok23fb3rXZLOSCWmrolpjw9t2nZWE6blpUyzyCWckAs5YBYaupqkLL+db3pmqStdUbGM4ilHBBLTfUS06+LU3zTPINYygGxlANiKQfEUg6IpRwQSzkglnJALOWAWMoBsVTdt9rPAieBa4rnbTCrY7mxyotq/drDxZNKX1f5yH0dPJaclxhLOSCWaiogcw2ddzEeS6KRGsS6w0uMpWoNiKQtko5KOiGp9p5mkvZKOiPpUOlYI834utIcsLaASFoBvAQ8AGwAthfN8Oq0D9jSd6ypZnzdaA7Y32htUhtwL/BBaX83sLuu85fOexNwqLR/FFhdPF8NHK17TMW53wU2t2U8/211LjFrgN9K+/PFsab9rxkfsGgzvknKmgM2MZ6yOgOy2EfKZ/4Sqr85YNPj6VdnQOaBdaX9tcCpGs+/lNNFEz7qbsaXNQdsYjyLqTMgB4H1km6WdAWwjV4jvKY10oyvM80Bay7EHgSOAT8DzzVQCB4AFoDz9Ga0ncDV9K4WjhePV9U0lk30ltjvge+K7cGmxrPU5juplvKdVEs5IJZyQCzlgFjKAbGUA2IpB8RSDoil/gWJVscCZN7T3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualization\n",
    "def visualize(num_examples=10, color_space='gray'):\n",
    "    examples = 0\n",
    "    index = 0\n",
    "    while examples < num_examples:\n",
    "        if train_labels[index] in [2, 3, 4]:\n",
    "            plt.figure()\n",
    "\n",
    "            # sp1\n",
    "            plt.subplot(211)\n",
    "            num1 = np.reshape(train_data[index], [28, 28])\n",
    "            plt.imshow(num1, cmap='gray', interpolation='nearest')\n",
    "            plt.title(train_labels[index])\n",
    "        \n",
    "            # sp2\n",
    "            plt.subplot(221)\n",
    "            num2 = np.reshape(bin_train_data[index], [28, 28])\n",
    "            plt.imshow(num2, cmap='gray', interpolation='nearest')\n",
    "            plt.title(train_labels[index])\n",
    "    \n",
    "            plt.show()\n",
    "        \n",
    "            examples += 1\n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b9da574d24193df76e96ed8ca62c7b0",
     "grade": false,
     "grade_id": "cell-56b33654497d4052",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2 Implementation (40 points)\n",
    "You are going to write a function ```EM(X, K, max_iter)``` that implements the EM algorithm on the Bernoulli mixture model. \n",
    "\n",
    "The only parameters the function has are:\n",
    "* ```X``` :: (NxD) array of input training images\n",
    "* ```K``` :: size of the latent space\n",
    "* ```max_iter``` :: maximum number of iterations, i.e. one E-step and one M-step\n",
    "\n",
    "You are free to specify your return statement.\n",
    "\n",
    "Make sure you use a sensible way of terminating the iteration process early to prevent unnecessarily running through all epochs. Vectorize computations using ```numpy``` as  much as possible.\n",
    "\n",
    "You should implement the `E_step(X, mu, pi)` and `M_step(X, gamma)` separately in the functions defined below. These you can then use in your function `EM(X, K, max_iter)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "316c9131692747c363b5db8e9091d362",
     "grade": false,
     "grade_id": "cell-882b13c117a73cc4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "\n",
    "def E_step(X, mu, pi): # BISHOP 9.23\n",
    "    datapoints = np.shape(X)[0] # N\n",
    "    latent_vars = np.shape(pi)[0] # K\n",
    "\n",
    "    gamma = np.zeros([datapoints, latent_vars])\n",
    "    # loop through all datapoints\n",
    "    for n in range(datapoints):\n",
    "                \n",
    "        # calculate the normalization term (denominator)\n",
    "        normalization = 0\n",
    "        for j in range(latent_vars):\n",
    "            normalization += pi[j] * np.prod(bernoulli.pmf(X[n], mu[j]))\n",
    "\n",
    "        # calculate the numerator for each latent variable and divide by normalization\n",
    "        for k in range(latent_vars):\n",
    "            numerator = pi[j] * np.prod(bernoulli.pmf(X[n], mu[k]))\n",
    "            gamma[n, k] = numerator / normalization\n",
    "            \n",
    "    return gamma # datapoints x latent variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1418f4014e98024fc97446ce27766c1d",
     "grade": true,
     "grade_id": "cell-f7c7dd52d82e2498",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000117639742616\n",
      "0.49709714956905443\n",
      "0.4999494209642031\n",
      "0.499932648580972\n",
      "0.4986264735059682\n"
     ]
    }
   ],
   "source": [
    "# Let's test on 5 datapoints\n",
    "n_test = 5\n",
    "X_test = bin_train_data[:n_test]\n",
    "D_test, K_test = X_test.shape[1], 10\n",
    "\n",
    "np.random.seed(2018)\n",
    "mu_test = np.random.uniform(low=.25, high=.75, size=(K_test,D_test))\n",
    "pi_test = np.ones(K_test) / K_test\n",
    "\n",
    "gamma_test = E_step(X_test, mu_test, pi_test)\n",
    "assert gamma_test.shape == (n_test, K_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2c426a613653174795cd9c8327ab6e20",
     "grade": false,
     "grade_id": "cell-f1b11b8765bd1ef6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    mu = (1 / np.sum(gamma) * (X.T @ gamma)).T\n",
    "    pi = np.sum(gamma, 0) / np.shape(X)[0]\n",
    "    return mu, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f60d48b8b22063cef560b42944a0aa4",
     "grade": true,
     "grade_id": "cell-6e7c751b30acfd45",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Oh, let's test again\n",
    "mu_test, pi_test = M_step(X_test, gamma_test)\n",
    "\n",
    "assert mu_test.shape == (K_test,D_test)\n",
    "assert pi_test.shape == (K_test, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "acfec6384b058cb0ce1932006fbfebc4",
     "grade": true,
     "grade_id": "cell-d6c4368246dee7e6",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twee xannies 0\n",
      "0.5016076482324686\n",
      "0.4987453428302743\n",
      "0.5006586197852809\n",
      "0.5009417405494424\n",
      "0.49914280739680555\n",
      "0.4991306475114456\n",
      "0.4996572385718494\n",
      "0.49990765309860685\n",
      "0.4987687169178561\n",
      "0.49872355894402404\n",
      "0.5018453426112781\n",
      "0.4980148219231761\n",
      "0.5011976730563449\n",
      "0.49937719033651345\n",
      "0.498858569834711\n",
      "0.4987961117934684\n",
      "0.49884292771862027\n",
      "0.49973992622531793\n",
      "0.5001928200471835\n",
      "0.4995152473290057\n",
      "0.4991653733287358\n",
      "0.49933807801364094\n",
      "0.4982371376169832\n",
      "0.5006125915688434\n",
      "0.4990208097747523\n",
      "0.5007066016625978\n",
      "0.5000164888815409\n",
      "0.49983272875175055\n",
      "0.49871656372986933\n",
      "0.49884796398667414\n",
      "0.5007136162321015\n",
      "0.49756071494672605\n",
      "0.4997993543615823\n",
      "0.4990352471119074\n",
      "0.49980916487584526\n",
      "0.49920764904553255\n",
      "0.4999241267516702\n",
      "0.49808876333993907\n",
      "0.4995696005367522\n",
      "0.49844070184611233\n",
      "0.49852995125522465\n",
      "0.5009568979651375\n",
      "0.49952875540901737\n",
      "0.4999469652623896\n",
      "0.5005422791377276\n",
      "0.4985466005636624\n",
      "0.5010096695063643\n",
      "0.49928061614545305\n",
      "0.5015539857505351\n",
      "0.500814447195025\n",
      "0.5020238785953229\n",
      "0.5007146192808256\n",
      "0.4994413746747038\n",
      "0.5003092025391103\n",
      "0.49879348158737774\n",
      "0.5004539111131987\n",
      "0.5011891368267245\n",
      "0.49869482747240507\n",
      "0.5001288525027157\n",
      "0.5005613151328163\n",
      "0.4999177799951092\n",
      "0.5007579385923953\n",
      "0.4998762334983725\n",
      "0.49968664214950465\n",
      "0.49881701349354146\n",
      "0.49834990251584815\n",
      "0.5008547648902694\n",
      "0.5000470163348771\n",
      "0.49949729692908285\n",
      "0.500200940108961\n",
      "0.5000685268739773\n",
      "0.4980342724617242\n",
      "0.49830938517375767\n",
      "0.49929813185041416\n",
      "0.5011590388677942\n",
      "0.5000396149519254\n",
      "0.5020339229422024\n",
      "0.5000928130798312\n",
      "0.4990324907295832\n",
      "0.49815669266651164\n",
      "0.501357474016022\n",
      "0.4998056962412398\n",
      "0.49947995111474197\n",
      "0.4995618041812924\n",
      "0.49928050717444306\n",
      "0.5005358563903195\n",
      "0.49939712974285877\n",
      "0.4988851754428274\n",
      "0.500702805999441\n",
      "0.4988922368026829\n",
      "0.49933335022105396\n",
      "0.49776733978365384\n",
      "0.49985796272190525\n",
      "0.4990927035075013\n",
      "0.5003950479306531\n",
      "0.4993660571861034\n",
      "0.49893668249618955\n",
      "0.5010868358455967\n",
      "0.5016894977763021\n",
      "0.4996766382101682\n",
      "twee xannies 1\n",
      "0.818359698660467\n",
      "0.793943909269926\n",
      "0.8489410433566469\n",
      "0.8858135939699571\n",
      "0.8462142328964142\n",
      "0.7997219830849223\n",
      "0.8847319765291265\n",
      "0.7756123922111562\n",
      "0.9196604354056853\n",
      "0.8477677564633077\n",
      "0.8192500808093326\n",
      "0.8934869661853919\n",
      "0.7699388155160993\n",
      "0.804489834641045\n",
      "0.9207905317371775\n",
      "0.8319208962586219\n",
      "0.8306529964576405\n",
      "0.8239975064844103\n",
      "0.8864360230841359\n",
      "0.8719667510596838\n",
      "0.7793621927180007\n",
      "0.7762041598534549\n",
      "0.8844131230182655\n",
      "0.8870431832069917\n",
      "0.8444080952569826\n",
      "0.7463496298280835\n",
      "0.8951375498796439\n",
      "0.7195538035402097\n",
      "0.7498036586072996\n",
      "0.8852294315393971\n",
      "0.823964206096656\n",
      "0.7962959836254357\n",
      "0.864379926645701\n",
      "0.857760151583693\n",
      "0.7791455890237252\n",
      "0.8836451916649964\n",
      "0.8097959715125446\n",
      "0.7724108771110292\n",
      "0.8505764052817079\n",
      "0.8326224545630564\n",
      "0.9063059116191817\n",
      "0.8125103123737594\n",
      "0.9039921016751105\n",
      "0.874415731686055\n",
      "0.8669182011865835\n",
      "0.8415943542406648\n",
      "0.8287755239822524\n",
      "0.8545968204166712\n",
      "0.8700202666525335\n",
      "0.7929108515821633\n",
      "0.8615381344034131\n",
      "0.7236429622331656\n",
      "0.8165968781570557\n",
      "0.8714692375295784\n",
      "0.8387422534449352\n",
      "0.7960587801655518\n",
      "0.749875044120281\n",
      "0.8613060633486445\n",
      "0.7993647882379029\n",
      "0.8881911412370465\n",
      "0.808657933797971\n",
      "0.8675126392025017\n",
      "0.811734773329162\n",
      "0.7184746842262029\n",
      "0.8242065115310007\n",
      "0.8795074214930425\n",
      "0.8308010057135641\n",
      "0.8795177256733169\n",
      "0.8335966638784007\n",
      "0.7507068427934227\n",
      "0.8562141906632199\n",
      "0.856257545352839\n",
      "0.9187089141118877\n",
      "0.8202474623024387\n",
      "0.8263979818445704\n",
      "0.7861021557409479\n",
      "0.8128290209812762\n",
      "0.885129945626985\n",
      "0.869538183307587\n",
      "0.8471019687256881\n",
      "0.8191173236399956\n",
      "0.7662348697538628\n",
      "0.7510383890484309\n",
      "0.8196428485547865\n",
      "0.8664743052238337\n",
      "0.8219158491161179\n",
      "0.847665594333082\n",
      "0.8151964867764948\n",
      "0.7718902731580738\n",
      "0.8241096090028983\n",
      "0.8079464116675986\n",
      "0.8297856602510146\n",
      "0.8778977654963199\n",
      "0.8150000552978492\n",
      "0.8192577290940775\n",
      "0.7807654560943181\n",
      "0.8757957957833812\n",
      "0.8105255893655317\n",
      "0.8528162502847362\n",
      "0.9051324372694312\n",
      "twee xannies 2\n",
      "0.8123683866478433\n",
      "0.788131321090331\n",
      "0.8427261279795107\n",
      "0.8793286294827821\n",
      "0.8400190520462052\n",
      "0.7938670807001108\n",
      "0.8782549156122422\n",
      "0.7699338964639282\n",
      "0.9129277710123285\n",
      "0.8415612496004037\n",
      "0.8132521746827535\n",
      "0.8869459206246909\n",
      "0.7643019634687553\n",
      "0.7986000318412356\n",
      "0.914049599438951\n",
      "0.8258303280302413\n",
      "0.824571752915436\n",
      "0.8179648671258183\n",
      "0.879946521270079\n",
      "0.8655830657160878\n",
      "0.773656442708561\n",
      "0.7705214002926252\n",
      "0.8779383725149059\n",
      "0.8805492193319359\n",
      "0.8382261523334713\n",
      "0.7408853774267674\n",
      "0.8885843471206931\n",
      "0.7142856349901806\n",
      "0.7443141715741143\n",
      "0.8787487226847488\n",
      "0.8179319469533803\n",
      "0.7904661426033607\n",
      "0.8580518742663948\n",
      "0.8514804413931436\n",
      "0.7734411929390405\n",
      "0.8771761370446833\n",
      "0.803867307595492\n",
      "0.7667558045198092\n",
      "0.8443493604799592\n",
      "0.8265267224771013\n",
      "0.899670949638866\n",
      "0.806561721592915\n",
      "0.8973741135650779\n",
      "0.8680141159893058\n",
      "0.8605715567663934\n",
      "0.8354329767966637\n",
      "0.8227079476470052\n",
      "0.8483403346964107\n",
      "0.8636508870725935\n",
      "0.7871057217956112\n",
      "0.855230842818539\n",
      "0.7183449825415709\n",
      "0.8106185369841974\n",
      "0.8650892587116918\n",
      "0.8326018162096867\n",
      "0.7902306345860213\n",
      "0.7443850770481049\n",
      "0.8550003996834218\n",
      "0.7935125527168022\n",
      "0.8816887741728369\n",
      "0.8027378824640765\n",
      "0.8611615929231857\n",
      "0.8057918889604259\n",
      "0.7132144533657349\n",
      "0.8181725194284721\n",
      "0.8730686749528398\n",
      "0.824718628780693\n",
      "0.8730788042694035\n",
      "0.8274940782584507\n",
      "0.7452106209235375\n",
      "0.8499458408581507\n",
      "0.8499888907691563\n",
      "0.9119832066808021\n",
      "0.8142423213237636\n",
      "0.8203478029228309\n",
      "0.7803468952497123\n",
      "0.806878264884537\n",
      "0.8786499612447926\n",
      "0.8631722741314519\n",
      "0.8409003078094199\n",
      "0.8131204927583091\n",
      "0.7606249993862603\n",
      "0.7455398258855951\n",
      "0.8136421557625891\n",
      "0.8601309288171121\n",
      "0.8158984742488355\n",
      "0.8414599559342635\n",
      "0.8092282800923294\n",
      "0.7662392065754743\n",
      "0.8180762904466916\n",
      "0.8020313580776444\n",
      "0.8237107560506248\n",
      "0.8714707656723707\n",
      "0.8090332894652312\n",
      "0.8132597717540392\n",
      "0.7750492313553903\n",
      "0.8693841142155754\n",
      "0.8045915578803254\n",
      "0.8465727583900792\n",
      "0.8985060713820917\n",
      "twee xannies 3\n",
      "0.8183722032828\n",
      "0.7939560137373262\n",
      "0.8489543034051645\n",
      "0.8858273160151623\n",
      "0.8462272209034286\n",
      "0.7997341632849692\n",
      "0.8847456668283186\n",
      "0.7756241008678467\n",
      "0.9196747721732816\n",
      "0.8477808160798149\n",
      "0.8192625230144238\n",
      "0.8935009026991856\n",
      "0.769950544755723\n",
      "0.8045020933090516\n",
      "0.920804891488781\n",
      "0.8319336350687931\n",
      "0.8306657585455357\n",
      "0.8240100445632824\n",
      "0.8864497745094786\n",
      "0.8719801655760401\n",
      "0.7793741581901583\n",
      "0.7762159466493277\n",
      "0.8844267844029519\n",
      "0.887056926647376\n",
      "0.8444210708461519\n",
      "0.7463608984567989\n",
      "0.8951514381328836\n",
      "0.7195645704883872\n",
      "0.7498150328883699\n",
      "0.8852431234118905\n",
      "0.8239768807910443\n",
      "0.7963080905068294\n",
      "0.8643933149117171\n",
      "0.8577733157611045\n",
      "0.7791573181409096\n",
      "0.8836589155226691\n",
      "0.8098082968671527\n",
      "0.7724225213171658\n",
      "0.8505895324824446\n",
      "0.8326351764046003\n",
      "0.9063199761989256\n",
      "0.8125226240718738\n",
      "0.904006165320465\n",
      "0.8744291826028562\n",
      "0.8669316190176355\n",
      "0.8416072523247162\n",
      "0.8287881788249767\n",
      "0.8546100021014675\n",
      "0.8700337072516027\n",
      "0.7929228346791375\n",
      "0.8615514347368497\n",
      "0.7236539187112901\n",
      "0.8166094212353733\n",
      "0.8714827089924682\n",
      "0.8387551679122369\n",
      "0.7960708419823567\n",
      "0.7498864625516529\n",
      "0.8613192883311818\n",
      "0.7993770149685584\n",
      "0.8882049033786962\n",
      "0.8086705243734447\n",
      "0.8675260158585186\n",
      "0.8117471020133824\n",
      "0.7184854723609467\n",
      "0.8242192314343235\n",
      "0.8795210971711114\n",
      "0.830813719990906\n",
      "0.8795313014087228\n",
      "0.8336096810965209\n",
      "0.7507181079170094\n",
      "0.8562273737629275\n",
      "0.8562707417727768\n",
      "0.9187232270159875\n",
      "0.8202599872409708\n",
      "0.826410591461443\n",
      "0.7861140571327091\n",
      "0.8128415064972405\n",
      "0.8851436320951761\n",
      "0.8695515570244474\n",
      "0.8471149895756492\n",
      "0.8191298679254717\n",
      "0.7662464065026644\n",
      "0.7510497456393904\n",
      "0.8196553861624758\n",
      "0.8664877346815251\n",
      "0.8219283800904891\n",
      "0.8476787737036979\n",
      "0.8152088896231623\n",
      "0.7719021051989676\n",
      "0.8241222911770116\n",
      "0.8079587788000265\n",
      "0.8297983983485381\n",
      "0.8779113786878605\n",
      "0.8150124580491369\n",
      "0.8192701761926322\n",
      "0.7807772408617726\n",
      "0.8758093057398431\n",
      "0.8105378998563917\n",
      "0.8528293626382205\n",
      "0.9051464888735762\n",
      "twee xannies 4\n",
      "0.8183596919337941\n",
      "0.7939438756652556\n",
      "0.8489413245142181\n",
      "0.885813773406231\n",
      "0.8462142837043689\n",
      "0.7997219368760211\n",
      "0.884732140755739\n",
      "0.7756122430557354\n",
      "0.9196607121013486\n",
      "0.8477678551292502\n",
      "0.8192499980541285\n",
      "0.8934872427757602\n",
      "0.7699387736814558\n",
      "0.8044897940075539\n",
      "0.9207908141394806\n",
      "0.8319209163913794\n",
      "0.8306530592515338\n",
      "0.823997447022444\n",
      "0.8864362223843445\n",
      "0.8719668346636098\n",
      "0.7793622430468284\n",
      "0.7762040797890236\n",
      "0.8844132632054759\n",
      "0.8870433652400451\n",
      "0.8444081612596791\n",
      "0.7463494880231991\n",
      "0.8951377529759217\n",
      "0.7195535697194974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749803569647648\n",
      "0.8852295897341492\n",
      "0.8239642837572169\n",
      "0.7962959164759955\n",
      "0.8643800999877522\n",
      "0.8577602020442922\n",
      "0.7791454063126496\n",
      "0.8836454060644426\n",
      "0.8097959164439306\n",
      "0.7724107124510947\n",
      "0.8505765285919668\n",
      "0.8326224470019581\n",
      "0.9063061202963334\n",
      "0.8125102021517694\n",
      "0.903992344791625\n",
      "0.8744158142496299\n",
      "0.8669183652878458\n",
      "0.8415943857561583\n",
      "0.8287755082355851\n",
      "0.8545969367456827\n",
      "0.8700204060968045\n",
      "0.7929107124024033\n",
      "0.8615382632598083\n",
      "0.7236428554240703\n",
      "0.81659693683594\n",
      "0.8714693856851996\n",
      "0.8387423449466059\n",
      "0.7960586715785998\n",
      "0.7498749982189069\n",
      "0.8613061204032166\n",
      "0.7993647940197268\n",
      "0.888191324420992\n",
      "0.8086581613445921\n",
      "0.8675127530415362\n",
      "0.8117346919495296\n",
      "0.7184744880894075\n",
      "0.8242066306954167\n",
      "0.8795076509722297\n",
      "0.8308010184348557\n",
      "0.8795178550538382\n",
      "0.8335969367955619\n",
      "0.7507066308699928\n",
      "0.8562142836806169\n",
      "0.8562576510274529\n",
      "0.9187091814913635\n",
      "0.8202474470313511\n",
      "0.8263979572208242\n",
      "0.7861020389491875\n",
      "0.8128290797020324\n",
      "0.8851300999384671\n",
      "0.8695382632408069\n",
      "0.8471020388043011\n",
      "0.8191173449932191\n",
      "0.7662346920576008\n",
      "0.7510382635222665\n",
      "0.8196428551960524\n",
      "0.8664744877378797\n",
      "0.8219158143743271\n",
      "0.847665814313166\n",
      "0.8151964266351848\n",
      "0.7718903042890656\n",
      "0.8241096919201367\n",
      "0.807946426652405\n",
      "0.8297857123148183\n",
      "0.877897957098502\n",
      "0.814999998064223\n",
      "0.8192576511153347\n",
      "0.7807653042679856\n",
      "0.8757959162871684\n",
      "0.8105255082789323\n",
      "0.8528163245050142\n",
      "0.9051326509113654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def EM(X, K, max_iter, mu=None, pi=None):\n",
    " \n",
    "    # Init\n",
    "    if mu == None:\n",
    "        mu = np.random.uniform(low=.25, high=.75, size=(K, np.shape(X)[1]))\n",
    "    if pi == None:\n",
    "        pi = np.ones(K) / K\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        print(\"twee xannies\", i)\n",
    "        # E-STEP\n",
    "        gamma = E_step(X, mu, pi)\n",
    "        #print(gamma[:10])\n",
    "        \n",
    "        # M_STEP\n",
    "        mu, pi = M_step(X, gamma)\n",
    "        #print(mu[0, 400:500])\n",
    "        #print(pi)\n",
    "    return gamma, mu, pi\n",
    "\n",
    "EM(bin_train_data[:100], 10, 5)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b4fc12faa0da660f7a4d9cc7deb41b25",
     "grade": false,
     "grade_id": "cell-e1077ed3b83489be",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.3 Three digits experiment (10 points)\n",
    "In analogue with Bishop $\\S9.3.3$, sample a training set consisting of only __binary__ images of written digits $2$, $3$, and $4$. Run your EM algorithm and show the reconstructed digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bdbce0fad0ed151063d4c489ce999e3e",
     "grade": true,
     "grade_id": "cell-477155d0264d7259",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "485543f4893938d2a9dc1c17d8221cbc",
     "grade": false,
     "grade_id": "cell-88c9664f995b1909",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Can you identify which element in the latent space corresponds to which digit? What are the identified mixing coefficients for digits $2$, $3$ and $4$, and how do these compare to the true ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ae7b5acea6089e2590059f90b0d0a0be",
     "grade": true,
     "grade_id": "cell-3680ae2159c48193",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98e04feb59a36867367b3027df9e226d",
     "grade": false,
     "grade_id": "cell-0891dda1c3e80e9a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.4 Experiments (20 points)\n",
    "Perform the follow-up experiments listed below using your implementation of the EM algorithm. For each of these, describe/comment on the obtained results and give an explanation. You may still use your dataset with only digits 2, 3 and 4 as otherwise computations can take very long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "439067186fa3ef1d7261a9bcf5a84ea6",
     "grade": false,
     "grade_id": "cell-06fe1b1355689928",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.1 Size of the latent space (5 points)\n",
    "Run EM with $K$ larger or smaller than the true number of classes. Describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "791512aeadd30c4b586b966ca10e6fad",
     "grade": true,
     "grade_id": "cell-6c9057f2546b7215",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e12e40c2d2165e3bb500b5504128910d",
     "grade": true,
     "grade_id": "cell-f01c37653160244b",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b306681523a2e35eea310ac10bb68999",
     "grade": false,
     "grade_id": "cell-cf478d67239b7f2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.2 Identify misclassifications (10 points)\n",
    "How can you use the data labels to assign a label to each of the clusters/latent variables? Use this to identify images that are 'misclassified' and try to understand why they are. Report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "000c11bd8756a4e24296c7c55d3ee17e",
     "grade": true,
     "grade_id": "cell-daa1a492fbba5c7e",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "baf43434481c13d76ad51e3ba07e2bf5",
     "grade": true,
     "grade_id": "cell-329245c02df7850d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "640bc57a2d08c3becf534bb5e4b35971",
     "grade": false,
     "grade_id": "cell-67ce1222e8a7837b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4.3 Initialize with true values (5 points)\n",
    "Initialize the three classes with the true values of the parameters and see what happens. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a48f788e286458ef0f776865a3bcd58b",
     "grade": true,
     "grade_id": "cell-aa5d6b9f941d985d",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1dc4adf3081f3bec93f94c3b12b87db9",
     "grade": true,
     "grade_id": "cell-981e44f35a3764b0",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd613f41e5d2b7d22b0d5b1e7644a48a",
     "grade": false,
     "grade_id": "cell-19bfd7cf4017ed84",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Variational Auto-Encoder\n",
    "\n",
    "A Variational Auto-Encoder (VAE) is a probabilistic model $p(\\bx, \\bz)$ over observed variables $\\bx$ and latent variables and/or parameters $\\bz$. Here we distinguish the decoder part, $p(\\bx | \\bz) p(\\bz)$ and an encoder part $p(\\bz | \\bx)$ that are both specified with a neural network. A lower bound on the log marginal likelihood $\\log p(\\bx)$ can be obtained by approximately inferring the latent variables z from the observed data x using an encoder distribution $q(\\bz| \\bx)$ that is also specified as a neural network. This lower bound is then optimized to fit the model to the data. \n",
    "\n",
    "The model was introduced by Diederik Kingma (during his PhD at the UVA) and Max Welling in 2013, https://arxiv.org/abs/1312.6114. \n",
    "\n",
    "Since it is such an important model there are plenty of well written tutorials that should help you with the assignment. E.g: https://jaan.io/what-is-variational-autoencoder-vae-tutorial/.\n",
    "\n",
    "In the following, we will make heavily use of the torch module, https://pytorch.org/docs/stable/index.html. Most of the time replacing `np.` with `torch.` will do the trick, e.g. `np.sum` becomes `torch.sum` and `np.log` becomes `torch.log`. In addition, we will use `torch.FloatTensor()` as an equivalent to `np.array()`. In order to train our VAE efficiently we will make use of batching. The number of data points in a batch will become the first dimension of our data tensor, e.g. A batch of 128 MNIST images has the dimensions [128, 1, 28, 28]. To check check the dimensions of a tensor you can call `.size()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "92bd337f41c3f94777f47376c7149ca7",
     "grade": false,
     "grade_id": "cell-bcbe35b20c1007d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Loss function\n",
    "The objective function (variational lower bound), that we will use to train the VAE, consists of two terms: a log Bernoulli loss (reconstruction loss) and a KullbackLeibler divergence. We implement the two terms separately and combine them in the end.\n",
    "As seen in Part 1: Expectation Maximization, we can use a multivariate Bernoulli distribution to model the likelihood $p(\\bx | \\bz)$ of black and white images. Formally, the variational lower bound is maximized but in PyTorch we are always minimizing therefore we need to calculate the negative log Bernoulli loss and KullbackLeibler divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3fb5f70b132e1233983ef89d19998374",
     "grade": false,
     "grade_id": "cell-389d81024af846e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.1 Negative Log Bernoulli loss (5 points)\n",
    "The negative log Bernoulli loss is defined as,\n",
    "\n",
    "\\begin{align}\n",
    "loss = - (\\sum_i^D \\bx_i \\log \\hat{\\bx_i} + (1  \\bx_i) \\log(1  \\hat{\\bx_i})).\n",
    "\\end{align}\n",
    "\n",
    "Write a function `log_bernoulli_loss` that takes a D dimensional vector `x`, its reconstruction `x_hat` and returns the negative log Bernoulli loss. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "952435ca03f47ab67a7e88b8306fc9a0",
     "grade": false,
     "grade_id": "cell-1d504606d6f99145",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def log_bernoulli_loss(x_hat, x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd2a490aa694507bd032e86d77fc0087",
     "grade": true,
     "grade_id": "cell-9666dad0b2a9f483",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test test test\n",
    "x_test = torch.FloatTensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 0.9, 0.9, 0.9]])\n",
    "x_hat_test = torch.FloatTensor([[0.11, 0.22, 0.33, 0.44], [0.55, 0.66, 0.77, 0.88], [0.99, 0.99, 0.99, 0.99]])\n",
    "\n",
    "assert log_bernoulli_loss(x_hat_test, x_test) > 0.0\n",
    "assert log_bernoulli_loss(x_hat_test, x_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6b75b7a531ecc87bce57925c4da464ee",
     "grade": false,
     "grade_id": "cell-b3a7c02dee7aa505",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.2 Negative KullbackLeibler divergence (10 Points)\n",
    "The variational lower bound (the objective to be maximized) contains a KL term $D_{KL}(q(\\bz)||p(\\bz))$ that can often be calculated analytically. In the VAE we assume $q = N(\\bz, \\mu, \\sigma^2I)$ and $p = N(\\bz, 0, I)$. Solve analytically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d01a7e7fe2dcf5f1c5fb955b85c8a04a",
     "grade": true,
     "grade_id": "cell-4cab10fd1a636858",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "328115c94a66e8aba0a62896e647c3ba",
     "grade": false,
     "grade_id": "cell-c49899cbf2a49362",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Write a function `KL_loss` that takes two J dimensional vectors `mu` and `logvar` and returns the negative KullbackLeibler divergence. Where `logvar` is $\\log(\\sigma^2)$. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "33b14b79372dd0235d67bb66921cd3e0",
     "grade": false,
     "grade_id": "cell-125b41878005206b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def KL_loss(mu, logvar):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf72e196d2b60827e8e940681ac50a07",
     "grade": true,
     "grade_id": "cell-ba714bbe270a3f39",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test test test\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "assert KL_loss(mu_test, logvar_test) > 0.0\n",
    "assert KL_loss(mu_test, logvar_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "65335a588baac26bc48dd6c4d275fdca",
     "grade": false,
     "grade_id": "cell-18cb3f8031edec23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1.3 Putting the losses together (5 points)\n",
    "Write a function `loss_function` that takes a D dimensional vector `x`, its reconstruction `x_hat`, two J dimensional vectors `mu` and `logvar` and returns the final loss. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6ecb5b60b2c8d7b90070ed59320ee70",
     "grade": false,
     "grade_id": "cell-d2d18781683f1302",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "816e9508408bfcb2c7332b508d505081",
     "grade": true,
     "grade_id": "cell-57747988d29bbb5d",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "x_hat_test = torch.FloatTensor([[0.11, 0.22, 0.33], [0.44, 0.55, 0.66], [0.77, 0.88, 0.99]])\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "assert loss_function(x_hat_test, x_test, mu_test, logvar_test) > 0.0\n",
    "assert loss_function(x_hat_test, x_test, mu_test, logvar_test) < 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4506e06ed44a0535140582277a528ba4",
     "grade": false,
     "grade_id": "cell-9e3ba708967fe918",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 The model\n",
    "Below you see a data structure for the VAE. The modell itself consists of two main parts the encoder (images $\\bx$ to latent variables $\\bz$) and the decoder (latent variables $\\bz$ to images $\\bx$). The encoder is using 3 fully-connected layers, whereas the decoder is using fully-connected layers. Right now the data structure is quite empty, step by step will update its functionality. For test purposes we will initialize a VAE for you. After the data structure is completed you will do the hyperparameter search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "31eccf2f6600764e28eb4bc6c5634e49",
     "grade": false,
     "grade_id": "cell-e7d9dafee18f28a1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, fc1_dims, fc21_dims, fc22_dims, fc3_dims, fc4_dims):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*fc1_dims)\n",
    "        self.fc21 = nn.Linear(*fc21_dims)\n",
    "        self.fc22 = nn.Linear(*fc22_dims)\n",
    "        self.fc3 = nn.Linear(*fc3_dims)\n",
    "        self.fc4 = nn.Linear(*fc4_dims)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def decode(self, z):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # To be implemented\n",
    "        raise Exception('Method not implemented')\n",
    "\n",
    "VAE_test = VAE(fc1_dims=(784, 4), fc21_dims=(4, 2), fc22_dims=(4, 2), fc3_dims=(2, 4), fc4_dims=(4, 784))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a2243397998b4f55c25dfd734f3e7e0",
     "grade": false,
     "grade_id": "cell-c4f9e841b8972a43",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 Encoding (10 points)\n",
    "Write a function `encode` that gets a vector `x` with 784 elements (flattened MNIST image) and returns `mu` and `logvar`. Your function should use three fully-connected layers (`self.fc1()`, `self.fc21()`, `self.fc22()`). First, you should use `self.fc1()` to embed `x`. Second, you should use `self.fc21()` and `self.fc22()` on the embedding of `x` to compute `mu` and `logvar` respectively. PyTorch comes with a variety of activation functions, the most common calls are `F.relu()`, `F.sigmoid()`, `F.tanh()`. Make sure that your function works for batches of arbitrary size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "628bcd88c611cf01e70f77854600199b",
     "grade": false,
     "grade_id": "cell-93cb75b98ae76569",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def encode(self, x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "370d930fa9f10f1d3a451f3805c04d88",
     "grade": true,
     "grade_id": "cell-9648960b73337a70",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test, test, test\n",
    "VAE.encode = encode\n",
    "\n",
    "x_test = torch.ones((5,784))\n",
    "mu_test, logvar_test = VAE_test.encode(x_test)\n",
    "\n",
    "assert np.allclose(mu_test.size(), [5, 2])\n",
    "assert np.allclose(logvar_test.size(), [5, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f597cc2b5ef941af282d7162297f865",
     "grade": false,
     "grade_id": "cell-581b4ed1996be868",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Reparameterization (10 points)\n",
    "One of the major question that the VAE is answering, is 'how to take derivatives with respect to the parameters of a stochastic variable?', i.e. if we are given $\\bz$ that is drawn from a distribution $q(\\bz|\\bx)$, and we want to take derivatives. This step is necessary to be able to use gradient-based optimization algorithms like SGD.\n",
    "For some distributions, it is possible to reparameterize samples in a clever way, such that the stochasticity is independent of the parameters. We want our samples to deterministically depend on the parameters of the distribution. For example, in a normally-distributed variable with mean $\\mu$ and standard deviation $\\sigma$, we can sample from it like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\bz = \\mu + \\sigma \\odot \\epsilon,\n",
    "\\end{align}\n",
    "\n",
    "where $\\odot$ is the element-wise multiplication and $\\epsilon$ is sampled from $N(0, I)$.\n",
    "\n",
    "\n",
    "Write a function `reparameterize` that takes two J dimensional vectors `mu` and `logvar`. It should return $\\bz = \\mu + \\sigma \\odot \\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6331cb5dd23aaacbcf1a52cfecb1afaa",
     "grade": false,
     "grade_id": "cell-679aea8b2adf7ec4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def reparameterize(self, mu, logvar):\n",
    "            \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38d4e047717ab334b262c8c177f0a420",
     "grade": true,
     "grade_id": "cell-fdd7b27a3d17f84e",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test, test, test\n",
    "VAE.reparameterize = reparameterize\n",
    "VAE_test.train()\n",
    "\n",
    "mu_test = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "logvar_test = torch.FloatTensor([[0.01, 0.02], [0.03, 0.04], [0.05, 0.06]])\n",
    "\n",
    "z_test = VAE_test.reparameterize(mu_test, logvar_test)\n",
    "\n",
    "assert np.allclose(z_test.size(), [3, 2])\n",
    "assert z_test[0][0] < 5.0\n",
    "assert z_test[0][0] > -5.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9241ab0eaf8366c37ad57072ce66f095",
     "grade": false,
     "grade_id": "cell-0be851f9f7f0a93e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Decoding (10 points)\n",
    "Write a function `decode` that gets a vector `z` with J elements and returns a vector `x_hat` with 784 elements (flattened MNIST image). Your function should use two fully-connected layers (`self.fc3()`, `self.fc4()`). PyTorch comes with a variety of activation functions, the most common calls are `F.relu()`, `F.sigmoid()`, `F.tanh()`. Make sure that your function works for batches of arbitrary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8e833cfd7c54a9b67a38056d5d6cab8",
     "grade": false,
     "grade_id": "cell-bf92bb3878275a41",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def decode(self, z):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7732293fd7d971fcf255496e8c68638d",
     "grade": true,
     "grade_id": "cell-4abb91cb9e80af5d",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test test test\n",
    "VAE.decode = decode\n",
    "\n",
    "z_test = torch.ones((5,2))\n",
    "x_hat_test = VAE_test.decode(z_test)\n",
    "\n",
    "assert np.allclose(x_hat_test.size(), [5, 784])\n",
    "assert (x_hat_test <= 1).all()\n",
    "assert (x_hat_test >= 0).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2e113d1f45398b2a1399c336526e755",
     "grade": false,
     "grade_id": "cell-97511fbc4f5b469b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.6 Forward pass (10)\n",
    "To complete the data structure you have to define a forward pass through the VAE. A single forward pass consists of the encoding of an MNIST image $\\bx$ into latent space $\\bz$, the reparameterization of $\\bz$ and the decoding of $\\bz$ into an image $\\bx$.\n",
    "\n",
    "Write a function `forward` that gets a a vector `x` with 784 elements (flattened MNIST image) and returns a vector `x_hat` with 784 elements (flattened MNIST image), `mu` and `logvar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8b7433c4631dd01c07a5fe287e55ae13",
     "grade": false,
     "grade_id": "cell-26bb463b9f98ebd5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = x.view(-1, 784)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8e7e495f40465c162512e9873c360b25",
     "grade": true,
     "grade_id": "cell-347e5fba3d02754b",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test test test \n",
    "VAE.forward = forward\n",
    "\n",
    "x_test = torch.ones((5,784))\n",
    "x_hat_test, mu_test, logvar_test = VAE_test.forward(x_test)\n",
    "\n",
    "assert np.allclose(x_hat_test.size(), [5, 784])\n",
    "assert np.allclose(mu_test.size(), [5, 2])\n",
    "assert np.allclose(logvar_test.size(), [5, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a114a6fd781fb949b887e6a028e07946",
     "grade": false,
     "grade_id": "cell-62c89e4d3b253671",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.7 Training (15)\n",
    "We will now train the VAE using an optimizer called Adam, https://arxiv.org/abs/1412.6980. The code to train a model in PyTorch is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f3b6bb965fb48229c63cacda48baea65",
     "grade": false,
     "grade_id": "cell-be75f61b09f3b9b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train(epoch, train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data.view(-1, 784), mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48ca730dbef06a668f4dfdb24888f265",
     "grade": false,
     "grade_id": "cell-da1b063b7de850b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's train. You have to choose the hyperparameters. Make sure your loss is going down in a reasonable amount of epochs (around 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "846430258fb80f50b161135448726520",
     "grade": false,
     "grade_id": "cell-d4d4408d397f6967",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# fc1_dims = (?,?)\n",
    "# fc21_dims =\n",
    "# fc22_dims =\n",
    "# fc3_dims =\n",
    "# fc4_dims =\n",
    "# lr =\n",
    "# batch_size =\n",
    "# epochs =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b93390f399b743276bc25e67493344f2",
     "grade": true,
     "grade_id": "cell-ca352d8389c1809a",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains a hidden test, please don't delete it, thx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20719070ed85964de9722acc3456a515",
     "grade": false,
     "grade_id": "cell-5c77370db7cec9f2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the box below to train the model using the hyperparameters you entered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38306be3638e85812bd5b2a052fcc0a4",
     "grade": false,
     "grade_id": "cell-5712d42de1068398",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "# Load data\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size, shuffle=True, **{})\n",
    "\n",
    "# Init model\n",
    "VAE_MNIST = VAE(fc1_dims=fc1_dims, fc21_dims=fc21_dims, fc22_dims=fc22_dims, fc3_dims=fc3_dims, fc4_dims=fc4_dims)\n",
    "\n",
    "# Init optimizer\n",
    "optimizer = optim.Adam(VAE_MNIST.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, train_loader, VAE_MNIST, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2f8fcc9384e30cb154cf931f223898b",
     "grade": false,
     "grade_id": "cell-bd07c058c661b9c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the box below to check if the model you trained above is able to correctly reconstruct images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "80d198e03b1287741d761a12e38dcf73",
     "grade": false,
     "grade_id": "cell-df03d717307a6863",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Let's check if the reconstructions make sense\n",
    "# Set model to test mode\n",
    "VAE_MNIST.eval()\n",
    "    \n",
    "# Reconstructed\n",
    "train_data_plot = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor())\n",
    "\n",
    "train_loader_plot = torch.utils.data.DataLoader(train_data_plot,\n",
    "                                           batch_size=1, shuffle=False, **{})\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader_plot):\n",
    "    x_hat, mu, logvar = VAE_MNIST(data)\n",
    "    plt.imshow(x_hat.view(1,28,28).squeeze().data.numpy(), cmap='gray')\n",
    "    plt.title('%i' % train_data.train_labels[batch_idx])\n",
    "    plt.show()\n",
    "    if batch_idx == 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7f559122b150f5f1228d6b66b62f462c",
     "grade": false,
     "grade_id": "cell-76649d51fdf133dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.8 Visualize latent space (20 points)\n",
    "Now, implement the auto-encoder now with a 2-dimensional latent space, and train again over the MNIST data. Make a visualization of the learned manifold by using a linearly spaced coordinate grid as input for the latent space, as seen in  https://arxiv.org/abs/1312.6114 Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c879ffdb0d355349d7144a33d16ca93a",
     "grade": true,
     "grade_id": "cell-4a0af6d08d055bee",
     "locked": false,
     "points": 20,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9eb1684d646eea84a25638d184bfbda",
     "grade": false,
     "grade_id": "cell-dc5e1247a1e21009",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.8 Amortized inference (10 points)\n",
    "What is amortized inference? Where in the code of Part 2 is it used? What is the benefit of using it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "364ed922da59070f319d0bdfb0e41d92",
     "grade": true,
     "grade_id": "cell-6f7808a9b0098dbf",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
